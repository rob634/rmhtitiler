# V0.8 Production Readiness Review

**Version:** 0.8.13.3
**Date:** 2026-02-13
**Purpose:** Systematic review of each major component for production readiness before sharing with other teams for deployment.

> **Scope:** All production components. H3 Explorer and DuckDB are excluded (prototyping).

---

## Table of Contents

1. [Review Summary](#review-summary)
2. [Component Reviews](#component-reviews)
   - [A. Application Entry & Lifecycle](#a-application-entry--lifecycle)
   - [B. Configuration Management](#b-configuration-management)
   - [C. Authentication — Storage OAuth](#c-authentication--storage-oauth)
   - [D. Authentication — PostgreSQL](#d-authentication--postgresql)
   - [E. Authentication — Admin (Azure AD)](#e-authentication--admin-azure-ad)
   - [F. Token Cache Layer](#f-token-cache-layer)
   - [G. Azure Auth Middleware](#g-azure-auth-middleware)
   - [H. Background Token Refresh](#h-background-token-refresh)
   - [I. Health & Readiness Probes](#i-health--readiness-probes)
   - [J. TiPG Integration (Vector/OGC)](#j-tipg-integration-vectorogc)
   - [K. STAC API Integration](#k-stac-api-integration)
   - [L. Planetary Computer Endpoints](#l-planetary-computer-endpoints)
   - [M. Admin Console & Refresh Webhook](#m-admin-console--refresh-webhook)
   - [N. Infrastructure (Telemetry, Logging, Latency)](#n-infrastructure-telemetry-logging-latency)
   - [O. Dockerfile & Dependency Management](#o-dockerfile--dependency-management)
   - [P. Documentation & Deployment Guides](#p-documentation--deployment-guides)
3. [Cross-Cutting Concerns](#cross-cutting-concerns)
4. [Recommended Cleanup Actions](#recommended-cleanup-actions)

---

## Review Summary

| Component | Status | Priority Issues |
|-----------|--------|-----------------|
| A. Entry & Lifecycle | Good | Minor: `app.py` creates redundant `app` at module level |
| B. Configuration | Good | Cleanup: unused `ENABLE_VERSIONED_ASSETS` refs in CLAUDE.md but no code |
| C. Storage OAuth | Good | Solid — dual sync/async pattern, correct locking |
| D. PostgreSQL Auth | Good | Solid — 3-mode support, MI credential caching |
| E. Admin Auth (Azure AD) | Good | Minor: `verify_aud=False` is intentional but should be documented |
| F. Token Cache | Good | Well-designed async-first cache |
| G. Auth Middleware | Review | Uses `BaseHTTPMiddleware` — see note |
| H. Background Refresh | Good | Clean task lifecycle |
| I. Health Probes | Good | Comprehensive, well-structured |
| J. TiPG Integration | Good | Solid pool refresh, degraded-mode support |
| K. STAC API | Good | Pool sharing with TiPG is clean |
| L. Planetary Computer | Review | Custom tile renderer — potential cleanup target |
| M. Admin Console | Good | HTMX auto-refresh, webhook auth |
| N. Infrastructure | Good | Zero-overhead observability design |
| O. Dockerfile & Deps | Review | Dependency drift between Dockerfile and requirements.txt |
| P. Documentation | Review | Some docs reference older versions, need refresh |

---

## Component Reviews

### A. Application Entry & Lifecycle

**Files:** `geotiler/main.py`, `geotiler/app.py`, `geotiler/__init__.py`

**What it does:** `main.py` handles the critical initialization order (telemetry BEFORE FastAPI import), then `app.py:create_app()` builds the FastAPI instance with all routers and middleware.

**Assessment: Good**

- Telemetry initialization order is correct and well-documented
- Lifespan handler cleanly manages startup/shutdown for all services
- Non-fatal database init (degraded mode) is the right production pattern
- Feature flags gate each optional component cleanly

**Issues to review:**

1. **Dual `app` instantiation** — `app.py` line 372 creates `app = create_app()` at module level. This means importing `geotiler.app` directly (bypassing `main.py`) skips telemetry setup. This is documented but could confuse deployers. Consider removing the module-level `app` in `app.py` and only keeping `main.py:app`.

2. **Logging configured twice** — `app.py` line 55-58 calls `logging.basicConfig()`, but `main.py` configures logging via `LoggerFactory.configure()`. When using `main.py` (production), the `basicConfig` in `app.py` runs second and may conflict. Should remove the `basicConfig` from `app.py` and rely solely on `main.py`'s `LoggerFactory` setup.

3. **`__main__.py`** — Exists but isn't referenced. Verify it's needed or remove.

---

### B. Configuration Management

**Files:** `geotiler/config.py`

**What it does:** Pydantic Settings with environment variable binding, feature flags, and parsed properties.

**Assessment: Good**

- Type-safe configuration with sensible defaults
- Properties parse comma-separated lists cleanly
- JSON env vars for sample URLs is a good pattern
- Constants extracted with descriptive names

**Issues to review:**

1. **Versioned assets config** — `CLAUDE.md` and docs reference `ENABLE_VERSIONED_ASSETS`, `RMHGEOAPI_POSTGRES_*` env vars, but no corresponding fields exist in `config.py` and no router implements versioned assets. Either this was removed or never implemented. Clean up references from CLAUDE.md and docs.

2. **`tipg_catalog_ttl` default mismatch** — `config.py` defaults to `60` seconds, but `CLAUDE.md` says default is `300`. The WIKI says `300` as well. The code is the source of truth, but documentation should match.

3. **Missing `CORS_ORIGINS` config** — The app relies on Azure APIM/CDN for CORS (documented in `app.py` comments). This is correct for production, but deployers running standalone may need CORS. Consider adding a `cors_origins` setting (disabled by default) with a note about when to use it.

---

### C. Authentication — Storage OAuth

**Files:** `geotiler/auth/storage.py`

**What it does:** Acquires Azure Storage OAuth tokens via Managed Identity or Azure CLI, configures GDAL and fsspec environment variables.

**Assessment: Good**

- Async lock prevents thundering herd on token refresh
- `asyncio.to_thread()` correctly wraps blocking Azure SDK calls
- Clear separation between sync (startup) and async (request) paths
- Good troubleshooting logging for common failure modes

**Issues to review:**

1. **`DefaultAzureCredential` instantiated on every call** — `_acquire_storage_token()` creates a new `DefaultAzureCredential()` each time. While the SDK handles caching internally, it's slightly wasteful. Consider creating it once at module level or caching it. Low priority since token refresh happens every 45 minutes.

2. **GDAL config via private API** — `configure_gdal_auth()` uses `rasterio._env.set_gdal_config()` (private module). This works but could break on rasterio version bumps. The fallback to env vars is correct, so this is just a warning to monitor.

---

### D. Authentication — PostgreSQL

**Files:** `geotiler/auth/postgres.py`

**What it does:** Three-mode PostgreSQL auth (password, Key Vault, Managed Identity) with token caching.

**Assessment: Good**

- Clean mode dispatch pattern
- MI credential acquisition supports both user-assigned and system-assigned MI
- URL encoding of passwords prevents connection string issues
- `search_path` support for multi-schema deployments

**Issues to review:**

1. **Same `DefaultAzureCredential` pattern as storage** — instantiated on every call. Same low-priority note as above.

2. **Key Vault error handling** — `_get_password_from_keyvault()` raises on failure but the caller in `app.py` catches it and enters degraded mode. This is correct, but the return type annotation says `str` when it can raise. Consider `-> str` with documented raises, or `-> Optional[str]` with None returns.

---

### E. Authentication — Admin (Azure AD)

**Files:** `geotiler/auth/admin_auth.py`

**What it does:** Validates JWT tokens from Azure AD for admin endpoint protection. Uses JWKS endpoint for key discovery.

**Assessment: Good**

- `lru_cache` on JWKS client is correct (keys are stable)
- Multiple Azure AD claim formats handled (azp, appid, app_id)
- Proper `WWW-Authenticate: Bearer` response headers
- Clean fallback when auth is disabled (local dev)

**Issues to review:**

1. **`verify_aud=False`** — Audience verification is disabled because the code checks `azp`/`appid` directly instead. This is a valid approach for service-to-service MI tokens where the audience may be the generic `https://management.azure.com` or the app's own ID. Add a brief inline comment explaining why this is safe.

2. **No JWKS key rotation handling** — `PyJWKClient` with `cache_keys=True` caches signing keys. If Azure AD rotates keys, the cache may serve stale keys. `PyJWKClient` handles this with a `lifespan` parameter (default 300s). Current behavior is fine but verify the default lifespan is adequate for your key rotation frequency.

---

### F. Token Cache Layer

**Files:** `geotiler/auth/cache.py`

**What it does:** Dual-lock cache (threading.Lock for sync startup, asyncio.Lock for async requests) for OAuth tokens and error tracking.

**Assessment: Good**

- Clean separation of locked/unlocked methods
- `ErrorCache` for health reporting is a good pattern
- `get_status()` provides health endpoint data without exposing tokens

**Issues to review:**

1. **`_async_lock` created at dataclass init** — `asyncio.Lock` created at field init time, not inside a running event loop. This works in modern Python (3.10+) but on Python 3.9 it would raise a deprecation warning. The base image uses Python 3.11, so this is fine, but note the minimum Python version dependency.

---

### G. Azure Auth Middleware

**Files:** `geotiler/middleware/azure_auth.py`

**What it does:** Ensures Azure Storage OAuth is configured before each request by calling `get_storage_oauth_token_async()` and setting GDAL/fsspec env vars.

**Assessment: Review**

- `BaseHTTPMiddleware` from Starlette has known limitations — it wraps responses in a `StreamingResponse`, which can cause issues with background tasks and memory for large responses. For this use case (just setting env vars), it's probably fine, but worth noting.

**Issues to review:**

1. **Runs on every request** — Including health probes, static files, admin pages, etc. The middleware calls `get_storage_oauth_token_async()` on every request even though the token is cached and valid for ~45 minutes. The cache hit path is fast (just a datetime comparison), so this is acceptable overhead, but consider skipping paths that don't need storage auth (e.g., `/livez`, `/readyz`, `/health`, `/static/`, `/docs`, `/api`).

2. **`configure_gdal_auth` and `configure_fsspec_auth` on every request** — These set `os.environ` values that are already set. After the first request, they're no-ops in effect but still execute the code. Not a performance concern (env var writes are fast) but adds unnecessary log noise at debug level.

3. **Race condition on env vars** — `os.environ` writes are process-global. With a single uvicorn worker this is fine. If someone runs with `--workers > 1`, env vars would be per-process (fine). But if someone uses threads via `--workers 1 --loop asyncio` with thread pool, concurrent `os.environ` writes could theoretically interleave. In practice this doesn't matter since all writes set the same value. Just a note for documentation.

---

### H. Background Token Refresh

**Files:** `geotiler/services/background.py`

**What it does:** Runs every 45 minutes to proactively refresh Storage and PostgreSQL tokens, recreating connection pools with fresh credentials.

**Assessment: Good**

- Explicit `app` parameter (no globals) is clean
- Pool recreation for pgSTAC and TiPG handles MI token-in-connection-string correctly
- Non-fatal error handling — a failed refresh doesn't crash the app

**Issues to review:**

1. **No error backoff** — If token refresh fails repeatedly, it just logs and tries again in 45 minutes. For transient Azure AD outages, consider a shorter retry (e.g., 5 minutes) after failure before falling back to the regular 45-minute cycle.

2. **Task reference not stored** — `start_token_refresh()` returns the asyncio.Task but `app.py` doesn't store it. If the task raises an unhandled exception, it will be silently garbage collected. Consider storing it on `app.state.refresh_task` and adding an exception callback, or wrapping the while loop in a broader try/except.

---

### I. Health & Readiness Probes

**Files:** `geotiler/routers/health.py`, `geotiler/services/database.py`

**What it does:** Three-tier probe system: `/livez` (alive), `/readyz` (ready for traffic), `/health` (full diagnostics).

**Assessment: Good**

- `/livez` is instant (no I/O) — correct for liveness probes
- `/readyz` checks critical dependencies with proper 503 on failure
- `/health` provides rich diagnostics with service/dependency/hardware sections
- Database ping uses `asyncio.to_thread()` to avoid blocking
- Hardware info via psutil is useful for Azure App Service diagnostics

**Issues to review:**

1. **`/health` performs database ping on every call** — In production, monitoring systems may poll `/health` frequently. Each call does a `SELECT 1` round-trip. Consider adding caching (e.g., 10-second TTL) for the database ping result within the health endpoint.

2. **`_get_hardware_info()` calls `psutil.cpu_percent(interval=None)`** — With `interval=None`, this returns the CPU usage since the last call, which can be misleading (first call returns 0.0). Not a bug, but the value may confuse operators. Consider using `interval=0.1` for a short sample, or document the behavior.

---

### J. TiPG Integration (Vector/OGC)

**Files:** `geotiler/routers/vector.py`, `geotiler/routers/diagnostics.py`

**What it does:** Integrates TiPG for OGC Features API + Vector Tiles, manages asyncpg pool lifecycle, handles token refresh pool recreation.

**Assessment: Good**

- `TiPGStartupState` tracking is excellent for diagnostics
- Pool refresh correctly closes old pool before creating new one
- `search_path` includes both configured schemas and `public` (for PostGIS types)
- Diagnostics endpoint provides deep visibility into table discovery issues

**Issues to review:**

1. **`diagnostics.py` is 46KB** — This is a large file for a diagnostics endpoint. Consider whether all the SQL queries are needed in production, or if some should be gated behind a `verbose=true` query param (some already are, but the file is still very large).

2. **`register_collection_catalog` called during pool refresh** — This re-scans the database for PostGIS tables every time the token is refreshed (every 45 minutes). This is correct but could be slow if there are many tables. The TTL middleware handles this separately when enabled; having both could cause duplicate scans. Not a bug — just a note.

---

### K. STAC API Integration

**Files:** `geotiler/routers/stac.py`

**What it does:** Creates stac-fastapi-pgstac STAC API instance, shares asyncpg pool with TiPG.

**Assessment: Good**

- Pool sharing via `app.state.readpool`/`writepool` is clean
- `CoreCrudClient` accesses pool at request time (lazy), so startup order doesn't matter
- Module-level `_stac_api` reference is acceptable for `get_stac_api()` helper

**Issues to review:**

1. **Read-write pool aliased to same pool** — `app.state.writepool = app.state.pool`. If the PostgreSQL user is read-only (as suggested by "App Reader Identity" in docs), write operations will fail at query time. This is fine if STAC API is used read-only, but `search POST` may attempt writes (search registration). Verify the STAC API usage pattern matches the database permissions.

---

### L. Planetary Computer Endpoints

**Files:** `geotiler/routers/planetary_computer.py`

**What it does:** Custom endpoints for accessing Planetary Computer Zarr datasets with SAS token handling and tile rendering.

**Assessment: Review**

- Credential caching per storage account is correct
- URL parsing for PC storage accounts is thorough

**Issues to review:**

1. **Custom tile renderer** — The `/pc/tiles` endpoint implements its own tile rendering (matplotlib + PIL) rather than using TiTiler's built-in xarray pipeline. This duplicates functionality and won't benefit from TiTiler's optimizations (e.g., overview levels, proper reprojection). Consider whether this custom renderer is still needed, or if the `/xarray/tiles` endpoint with PC credential injection would be sufficient.

2. **Error responses return JSON with 200 status** — `/pc/variables` and `/pc/info` return `{"error": "..."}` with HTTP 200 instead of raising `HTTPException` with proper status codes. This makes error handling harder for API consumers. Consider returning proper 4xx/5xx responses.

3. **`open_pc_zarr_dataset()` opens full dataset for every tile** — Each tile request opens the entire Zarr store. There's no dataset caching. At low zoom levels this works, but at higher zoom levels with frequent tile requests, this creates significant overhead. Consider caching opened datasets with a TTL.

4. **Inline HTML fallback** — `_get_map_html()` has a full HTML page as a Python string. The template file exists at `templates/pc_map.html`, so the inline fallback may be dead code.

---

### M. Admin Console & Refresh Webhook

**Files:** `geotiler/routers/admin.py`

**What it does:** HTML dashboard at `/`, JSON API info at `/api`, HTMX auto-refresh fragment, and TiPG catalog refresh webhook.

**Assessment: Good**

- HTMX auto-refresh is clean (30-second polling)
- Webhook returns before/after diff (useful for ETL integration)
- Admin auth dependency properly applied

**Issues to review:**

1. **`/admin/refresh-collections` returns 200 even on error** — When the refresh fails (line 183-189), it returns `{"status": "error", ...}` with HTTP 200. Consumers checking status codes will think it succeeded. Consider returning 500 on failure.

---

### N. Infrastructure (Telemetry, Logging, Latency)

**Files:** `geotiler/infrastructure/telemetry.py`, `geotiler/infrastructure/logging.py`, `geotiler/infrastructure/latency.py`, `geotiler/infrastructure/middleware.py`

**What it does:** Azure Monitor OpenTelemetry integration, structured JSON logging, latency tracking decorators, and request timing middleware.

**Assessment: Good**

- Zero-overhead design (disabled by default) is correct for production
- `_is_observability_enabled()` check at top of each function is fast
- KQL query examples in docstrings are excellent for operators
- `_normalize_endpoint()` for metric aggregation prevents high-cardinality

**Issues to review:**

1. **`LoggerFactory.create_logger()` modifies global `LogRecordFactory`** — Each call to `create_logger()` replaces the global log record factory. If multiple components call this, only the last one's factory is active. This means the `component` attribute is only reliably set for the last component registered. Consider using a logging `Filter` instead of replacing the factory.

2. **`_is_observability_enabled()` reads env var on every call** — For the middleware (every request), this does an `os.environ.get()` lookup each time. Consider caching the result at module load time, with a note that changing the env var requires a restart.

---

### O. Dockerfile & Dependency Management

**Files:** `Dockerfile`, `Dockerfile.local`, `requirements.txt`, `docker-compose.yml`

**What it does:** Production and local dev container builds based on `titiler-pgstac:1.9.0`.

**Assessment: Review**

**Issues to review:**

1. **Dependency drift between Dockerfile and requirements.txt** — The Dockerfile `pip install` list includes packages not in `requirements.txt` (e.g., `stac-fastapi.pgstac>=4.0.0`, `duckdb>=1.0.0`) and vice versa (`requirements.txt` has `asyncpg`, `psycopg2-binary`, `buildpg`, `pygeofilter`, `ciso8601`, `starlette-cramjam`, `PyJWT`, `cryptography` that aren't in the Dockerfile). The Dockerfile is the actual build, so `requirements.txt` appears to be out of sync. **Action:** Reconcile — either the Dockerfile should `pip install -r requirements.txt` or `requirements.txt` should be updated to match what the Dockerfile installs.

2. **`Dockerfile.local` missing TiPG, STAC, DuckDB, PyJWT** — Local dev Dockerfile doesn't install `tipg`, `stac-fastapi.pgstac`, `duckdb`, `PyJWT`, or `cryptography`. This means local dev can't test vector tiles, STAC API, admin auth, or H3 features. Consider making `Dockerfile.local` install the same packages as the production Dockerfile with only env vars differing.

3. **No `requirements.txt` pinning** — All dependencies use `>=` minimum versions. For reproducible builds, consider generating a `requirements-lock.txt` with pinned versions. This prevents a new upstream release from breaking a production build.

4. **No `.dockerignore`** — Without `.dockerignore`, `COPY geotiler /app/geotiler` copies `__pycache__/` directories into the image. Add a `.dockerignore` that excludes `__pycache__`, `*.pyc`, `.git`, `docs/`, `*.md`, test files, etc.

---

### P. Documentation & Deployment Guides

**Files:** `CLAUDE.md`, `docs/WIKI.md`, `docs/QA_DEPLOYMENT.md`, `docs/NEW_TENANT_DEPLOYMENT.md`, `README.md`, others

**Assessment: Review**

**Issues to review:**

1. **CLAUDE.md references `--docker-custom-image-name`** — This flag is deprecated. The memory file correctly notes to use `--container-image-name` instead. Update CLAUDE.md.

2. **WIKI.md version says 0.3.1** — Outdated. Should reflect current version or remove the version field.

3. **Versioned Assets references** — `CLAUDE.md` documents `ENABLE_VERSIONED_ASSETS` and `RMHGEOAPI_POSTGRES_*` env vars, but these don't appear to be implemented in the current codebase. Either the feature was removed or is in a separate branch. Clean up references, or add a note that it's planned.

4. **`docs/VERSIONED_ASSETS_IMPLEMENTATION.md`** — 31KB implementation doc for a feature that may not exist in this branch. Verify and either keep (if planned) or archive.

5. **Prototype files in root** — `preview_h3.py`, `palette_comparison.html`, `viewer.html`, `todo.md`, `MAP_VIEWER.md`, `diagram.png` are in the project root. Consider moving to `docs/` or `docs/archive/` before sharing with deployers.

---

## Cross-Cutting Concerns

### 1. Error Response Consistency

Several endpoints return errors as JSON with 200 status codes instead of proper HTTP error codes:
- `/pc/variables`, `/pc/info` — return `{"error": "..."}` with 200
- `/admin/refresh-collections` — returns `{"status": "error"}` with 200

**Recommendation:** Standardize on `HTTPException` or `JSONResponse(status_code=4xx/5xx)` for all error responses.

### 2. No Test Suite

There are no test files (`tests/`, `test_*.py`, `*_test.py`). For production distribution:
- Add at least smoke tests for health endpoints
- Add unit tests for auth token acquisition (mockable)
- Add unit tests for config parsing

### 3. No `setup.py` / `pyproject.toml`

The package has no Python packaging metadata. It's deployed as raw source (`COPY geotiler /app/geotiler`), which works for containers but:
- No dependency declaration outside Dockerfile
- Can't `pip install` the package
- No entry points

**Recommendation:** Add a minimal `pyproject.toml` with `[project]` metadata and dependencies. This enables `pip install -e .` for local dev and makes the Dockerfile cleaner (`pip install .` instead of copying + separate `pip install`).

### 4. Prototype Files Mixed with Production

Files that appear to be prototyping/testing artifacts:
- `preview_h3.py` (root)
- `palette_comparison.html` (root)
- `viewer.html` (root)
- `todo.md` (root)
- `MAP_VIEWER.md` (root)
- `geotiler/routers/h3_explorer.py` (H3 prototype)
- `geotiler/services/duckdb.py` (H3 prototype)
- `DUCKDB.md` (H3 docs)

**Recommendation:** Move H3/DuckDB to a feature branch or clearly mark as experimental. Move root prototype files to `docs/archive/`.

### 5. `__pycache__` in Repository

`__pycache__` directories and `.pyc` files appear to be tracked or at least present. Ensure `.gitignore` excludes them.

---

## Recommended Cleanup Actions

### Priority 1 — Must Fix Before Sharing

| # | Action | Files | Effort |
|---|--------|-------|--------|
| 1 | Remove `logging.basicConfig()` from `app.py` (conflicts with `main.py` logging) | `app.py:55-58` | 5 min |
| 2 | Add `.dockerignore` to exclude `__pycache__`, docs, prototype files | New file | 10 min |
| 3 | Reconcile Dockerfile and `requirements.txt` dependencies | `Dockerfile`, `requirements.txt` | 30 min |
| 4 | Fix `Dockerfile.local` to install all dependencies (TiPG, STAC, PyJWT, etc.) | `Dockerfile.local` | 15 min |
| 5 | Update `CLAUDE.md` — remove `--docker-custom-image-name`, remove versioned assets refs | `CLAUDE.md` | 15 min |
| 6 | Move root prototype files (`preview_h3.py`, `viewer.html`, etc.) to `docs/archive/` | Root files | 10 min |

### Priority 2 — Should Fix

| # | Action | Files | Effort |
|---|--------|-------|--------|
| 7 | Fix error response codes (PC endpoints, admin refresh) to use proper HTTP status | `planetary_computer.py`, `admin.py` | 30 min |
| 8 | Update `docs/WIKI.md` version field from 0.3.1 to current | `docs/WIKI.md` | 5 min |
| 9 | Fix `tipg_catalog_ttl` default documentation mismatch (code says 60, docs say 300) | `config.py` or docs | 5 min |
| 10 | Add comment explaining `verify_aud=False` in admin auth | `admin_auth.py` | 5 min |
| 11 | Store background refresh task reference on `app.state` | `app.py`, `background.py` | 10 min |
| 12 | Skip auth middleware for non-storage paths (`/livez`, `/health`, `/static/`) | `azure_auth.py` | 15 min |

### Priority 3 — Nice to Have

| # | Action | Files | Effort |
|---|--------|-------|--------|
| 13 | Add minimal `pyproject.toml` for proper Python packaging | New file | 30 min |
| 14 | Add smoke test suite (health endpoints, config parsing) | New `tests/` dir | 2-4 hrs |
| 15 | Cache `_is_observability_enabled()` result at module load | `latency.py`, `middleware.py` | 10 min |
| 16 | Fix `LoggerFactory.create_logger()` global factory replacement issue | `logging.py` | 30 min |
| 17 | Evaluate replacing custom PC tile renderer with `/xarray/tiles` + PC credentials | `planetary_computer.py` | Research |
| 18 | Add CORS configuration option (disabled by default) for standalone deployments | `config.py`, `app.py` | 30 min |
| 19 | Remove module-level `app = create_app()` from `app.py` | `app.py` | 10 min |
| 20 | Add `requirements-lock.txt` with pinned versions for reproducible builds | New file | 30 min |
